# ğŸŒ† ZenClarity-UrbanFlow â€” NYC Taxi Data Engineering Project

**A modern data engineering platform combining streaming + batch pipelines, dbt-powered transformations, and multi-engine analytics across Redshift Serverless, Snowflake, and EMR Spark**
Designed for **portability, cost-performance benchmarking, and real-time insights** delivered via Streamlit

- ğŸš– **Pipelines**: Streaming (Kinesis) + Batch (Glue, EMR Spark)
- ğŸ—„ï¸ **Data Lake & Warehouses**: S3 + Redshift, Snowflake, Spark SQL
- ğŸ“Š **dbt Modeling**: Staging â†’ Intermediate â†’ Marts
- ğŸŒ **Portability**: One dbt codebase across Redshift, Snowflake, EMR Spark SQL
- ğŸ§ª **Sanity Check**: Cross-engine runtime/cost notes in `/docs/benchmarks/`
- ğŸ“ˆ **Visualization**: Streamlit dashboard with KPIs & real-time vs baseline

---

## ğŸ—ºï¸ Architecture

![Architecture Diagram](docs/arch_diagrams/ZenClarity-UrbanFlow_architecture.jpg)

---

## ğŸŒ Portability â€” One dbt Codebase â†’ Three Engines

**One dbt codebase** runs on **Snowflake**, **Redshift**, and **EMR Spark**, enabling true engine flexibility with no rewrites. This design lets you benchmark cost/performance across platforms and keep models maintainable as a single source of truth

![Portability Overview](docs/arch_diagrams/portability_overview.jpg)

**Why it matters**
- Avoids **vendor lock-in** and simplifies migrations
- Enables **apples-to-apples benchmarking** across engines
- Keeps analytics **consistent and DRY** with shared models/macros

---

## ğŸ“Š Project Highlights

- **Data Ingestion**
  - *Streaming*: Python simulator + **Kinesis Firehose** for near real-time ingestion
  - *Batch*:
    - **AWS Glue** for serverless ETL.
    - **EMR Spark** jobs (PySpark/Hive) for scalable, distributed batch processing

- **Data Lake & Storage**: Central **Amazon S3** data lake; **DynamoDB** for observability and audit logging

- **Data Transformation**: ETL with **AWS Glue** and **EMR Spark**; ELT with **dbt** (multi-layer: staging, intermediate, marts)

- **Data Warehousing Engines**
  - **Redshift Serverless** for streaming & batch analytics
  - **Snowflake** for bulk loading + benchmarking
  - **EMR Spark SQL** for distributed queries and performance testing

- **Unified Transformation Layer**: A single **dbt project** runs seamlessly across all engines:
  - **Redshift** and **Snowflake** using native dbt adapters
  - **EMR Spark** via **Spark Thrift Server (STS)** with **Hive Metastore (Glue Catalog)** for schema management.
  This ensures portability, consistency, and reduced maintenance overhead

## Orchestration

- **AWS Step Functions** â€” production path that orchestrates **Glue** jobs to ingest from source and load into **Amazon Redshift**.

- **Apache Airflow (Docker, local)** â€” parallel/alternative path that triggers **EMR Spark** batch runs for heavier or custom Spark workloads.
  - DAG: `emr_ec2_submit_step` submits a `spark-submit` step to an existing EMR (EC2) cluster and waits for completion.
  - Config: AWS creds from host `~/.aws` (mounted read-only), defaults via Airflow **Variables**, run-time overrides via `dag_run.conf` (e.g. `{"year":"2024","month":"10","cab_type":"yellow"}`).

**Why both?** Step Functions powers the Glue-based production pipeline; Airflow demonstrates portability and scale-up options using EMR Spark, and serves as a developer-friendly orchestrator for iterative jobs.
flows

- **Visualization**: **Streamlit** surfaces KPIs and real-time vs baseline comparisons

---

## ğŸ“‚ Repo Structure

```text
ZenClarity-UrbanFlow/
â”œâ”€ analytics/
â”œâ”€ config/
â”œâ”€ dbt/
â”œâ”€ docs/
â”‚  â”œâ”€ arch_diagrams/
â”‚  â”œâ”€ benchmarks/
â”‚  â”œâ”€ metrics/
â”‚  â””â”€ runbooks/                 # gitignored
â”œâ”€ infrastructure/
â”‚  â”œâ”€ emr/                      
â”‚  â”œâ”€ glue/
â”‚  â”œâ”€ redshift/
â”‚  â””â”€ snowflake/
â”œâ”€ scripts/
â”‚  â”œâ”€ airflow/
â”‚  â”‚  â”œâ”€ emr_ec2_submit_step.py
â”‚  â”‚  â””â”€ vars_emr_ec2.json
â”‚  â”œâ”€ airflow_disabled/         # (ignored)
â”‚  â”œâ”€ batch/
â”‚  â”‚  â”œâ”€ glue_jobs/
â”‚  â”‚  â””â”€ emr_spark/
â”‚  â”œâ”€ emr_jobs/
â”‚  â”‚  â””â”€ emr_process_trip_data.py
â”‚  â”œâ”€ streaming/
â”‚  â””â”€ helpers/
â”œâ”€ tools/
â”‚  â””â”€ airflow-docker/
â”‚     â”œâ”€ docker-compose.yml
â”‚     â”œâ”€ Dockerfile
â”‚     â”œâ”€ logs/                  # ignored
â”‚     â””â”€ plugins/               # ignored
â””â”€ README.md

---

## ğŸ“ˆ Data Models & dbt

This project follows a **multi-layered dbt modeling pattern** for maintainable, scalable analytics code

- **Staging**: cleans raw data, enforces schema
- **Intermediate**: joins + transformations for readability/efficiency
- **Marts**: business-defined entities for analytics (facts & dims)

ğŸ“‘ **Documentation & lineage**: The entire dbt project is documented and includes a full lineage graph, showcasing the flow of data from source to dashboard
[View dbt Project Documentation (S3 Hosted)](http://nle-dbt-docs.s3-website-us-east-1.amazonaws.com/#!/overview)

---

## ğŸ“ˆ Dashboard KPIs (Streamlit)

- Trips count
- Total fare revenue
- Average trip delay
- Passengers carried
- Trips per minute
- Real-time vs baseline comparison
- Cumulative trip chart

**Screenshots:**
![Dashboard Screenshot](docs/metrics/streamlit_live_streaming_dashboard.jpg)

---

## ğŸŒ Technologies Used

**AWS Services:**
S3 â€¢ Kinesis Firehose â€¢ Glue â€¢ Lambda â€¢ Step Functions â€¢ EventBridge â€¢ DynamoDB â€¢ Athena â€¢ Redshift Serverless â€¢ EMR (Spark, Hive, STS)

**Other Tools:**
dbt â€¢ Snowflake â€¢ Python â€¢ Streamlit

---

## ğŸ“š Roadmap

- **Next Phase: Documentation & Tuning**
  - Extend docs with tuning experiments (partitioning, compaction, Spark optimizations)

- **Future Enhancements**
  - **AI-Powered Natural Language â†’ SQL Assistant (AWS-native)**
    - Explore **Amazon Bedrock** for NLâ†’SQL translation, grounded in dbt metadata + Glue Catalog
    - Use **Knowledge Bases** + **OpenSearch Serverless** for semantic context retrieval.
  - Iceberg tables / Athena for streaming cost optimization
  - Predictive analytics (surge demand zones)
  - More realistic simulation from historical patterns

---

## ğŸ’¡ Inspiration

> *â€œZenClarity-UrbanFlow embodies the idea that modern data engineering should empower everyone â€” from engineers to analysts â€” with scalable pipelines, portable models, and AI-driven access to insights.â€*

---

## ğŸ”— Connect

- LinkedIn: [le-nguyen-v](https://www.linkedin.com/in/le-nguyen-v/)
- GitHub: [tropily](https://github.com/tropily/ZenClarity-UrbanFlow)
