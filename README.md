# ðŸŒ† ZenClarity-UrbanFlow â€” NYC Taxi Data Engineering Project

**ZenClarity-UrbanFlow reflects my approach to data engineering: clarity, scalability, and modern cloud practices.**  
Built with AWS, dbt, Redshift, and Snowflake, it showcases both batch and streaming pipelines, automated orchestration, and a real-time analytics dashboard.

- ðŸš– **Pipelines**: Real-time streaming (Kinesis) + batch ingestion (Glue)  
- ðŸ“¦ **Warehousing**: Redshift Serverless & Snowflake  
- ðŸ“Š **Modeling**: dbt (staging â†’ intermediate â†’ marts)  
- âš¡ **Benchmarks**: Redshift vs Snowflake performance  
- ðŸ“ˆ **Visualization**: Streamlit dashboard (screenshots included)  

---

## ðŸ“Š Project Highlights

- **Data Ingestion**: Dual-pipeline system â€” Python-based simulator for live events via **Kinesis Firehose** and AWS Glue for scheduled batch ingestion.  
- **Data Lake & Storage**: Central **Amazon S3** data lake; **DynamoDB** used for observability and audit logging.  
- **Data Transformation**: ETL with **AWS Glue** and ELT with **dbt** (multi-layer: staging, intermediate, marts).  
- **Data Warehousing**:  
  - **Redshift Serverless** for streaming & batch analytics.  
  - **Snowflake** for bulk load + benchmarking.  
- **Orchestration**: **AWS Step Functions** automate and manage pipeline workflows.  
- **Visualization**: **Streamlit dashboard** surfaces KPIs and real-time vs baseline comparisons.  
- **Cross-Database Compatibility**: Custom **dbt macros** ensure the same models run seamlessly on both Redshift and Snowflake, enabling apples-to-apples benchmarking and portable analytics code.  

> Note: Streaming pipeline operates near-real-time (sub-minute latency) using fully managed AWS services.  


---

## ðŸ—ºï¸ Architecture

![Architecture Diagram](docs/arch_diagrams/ZenClarity-UrbanFlow_architecture.jpg)

---

## ðŸ“‚ Repo Structure 

ZenClarity-UrbanFlow/  
â”œâ”€â”€ analytics/          # notebooks + Streamlit app (screenshots archived)  
â”œâ”€â”€ config/             # sample env/config snippets  
â”œâ”€â”€ dbt/                # dbt models (staging â†’ intermediate â†’ marts)  
â”œâ”€â”€ docs/               # diagrams, metrics, planning notes  
â”œâ”€â”€ infrastructure/     # EMR, Glue, Redshift, Snowflake configs  
â”œâ”€â”€ scripts/            # ETL code (batch, streaming, emr_jobs, helpers)  
â””â”€â”€ venv/               # local virtualenv (ignored in git)  

---

## ðŸ“ˆ Data Models & dbt

This project follows a **multi-layered dbt modeling pattern**:
- **Staging**: cleans raw data, enforces schema.  
- **Intermediate**: joins + transformations for readability/efficiency.  
- **Marts**: business-defined entities for analytics (facts & dims).  

ðŸ“‘ **Documentation & lineage**:  
[View dbt Project Documentation (S3 Hosted)](http://nle-dbt-docs.s3-website-us-east-1.amazonaws.com/#!/overview)

---

## ðŸ“Š Performance Benchmarks

A representative query pack was executed on both **Redshift** and **Snowflake** using dbt-built schemas.  

- âœ… Filters & partition pruning  
- âœ… Aggregations & group-by  

Results highlight performance trade-offs between Redshift and Snowflake under equivalent schema and query conditions:

![Redshift vs Snowflake Benchmark](docs/metrics/Snowflake_vs_Redshift_Benchmark.jpg)

---

## ðŸ“ˆ Dashboard KPIs (Streamlit)

* Trip count  
* Total fare revenue  
* Average trip delay  
* Passengers carried  
* Trips per minute  
* Real-time vs baseline comparison  
* Cumulative trip chart  

**Screenshots:**  
![Dashboard KPIs](docs/metrics/Streaming-KPI.jpg)  
![Dashboard Screenshot](docs/metrics/Streaming-Dashboard-1.jpg)

---

## ðŸŒ Technologies Used

**AWS Services:**  
S3 â€¢ Kinesis Firehose â€¢ Glue â€¢ Lambda â€¢ Step Functions â€¢ EventBridge â€¢ DynamoDB â€¢ Athena â€¢ Redshift Serverless  

**Other Tools:**  
dbt â€¢ Snowflake â€¢ Python â€¢ Streamlit  

---

## ðŸ“š Roadmap

- **Next Phase: EMR Migration**  
  - Re-architect batch processing with Spark & Hive (EC2 + EMR Serverless).  
  - Compare cost & performance vs AWS Glue.  
  - Extend docs with tuning experiments.  

- **Future Enhancements**  
  - Iceberg tables / Athena for streaming cost optimization  
  - Predictive analytics (surge demand zones)  
  - More realistic simulation from historical patterns  

---

## ðŸ’¡ Inspiration

> *"Modern Data Engineering: Combining batch + streaming for near real-time decision making."*

---

## ðŸ”— Connect

- LinkedIn: [le-nguyen-v](https://www.linkedin.com/in/le-nguyen-v/)  
- GitHub: [tropily](https://github.com/tropily/Zen_Clarity)

